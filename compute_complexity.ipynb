{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: http://localhost:8000/evaluate\n",
      "Loaded results data from: results/tool-calling-o4-mini_range_0--1_user-task_finance-example-2_llm_2025-09-05_21-39-35.json\n",
      "Results contain 1 entries\n",
      "Sample result keys: ['task_id', 'reward', 'info', 'traj', 'trial']\n",
      "Prepared error evaluation request with inline data\n",
      "JSON response received\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "## NOTE: These examples won't work because the environments have been disabled in the API\n",
    "## but you can see the plots in the ./example folder\n",
    "# task_path = \"example/other_example.json\"\n",
    "\n",
    "task_path = \"example/finance_example_2.json\"\n",
    "\n",
    "with open(task_path, 'r') as file:\n",
    "    task_json = json.load(file)\n",
    "\n",
    "# Add the task file name to the request so the server can include it in the filename\n",
    "task_json['task_file_name'] = task_path\n",
    "\n",
    "# Configure number of trials/retries for task execution\n",
    "# Default is 1, but you can increase for more robust results\n",
    "task_json['num_trials'] = 1  # Change this to desired number of trials\n",
    "\n",
    "# API_BASE_URL= \"https://tau-bench.turing.com\"\n",
    "API_BASE_URL= \"http://localhost:8000\"\n",
    "\n",
    "# Choose which endpoint to use\n",
    "endpoint_options = {\n",
    "    \"compute_complexity\": f\"{API_BASE_URL}/compute_complexity\",\n",
    "    \"task_verification\": f\"{API_BASE_URL}/task_verification\", \n",
    "    \"run_task\": f\"{API_BASE_URL}/run-task\",\n",
    "    \"evaluate\": f\"{API_BASE_URL}/evaluate\"\n",
    "}\n",
    "\n",
    "# Select the endpoint you want to use\n",
    "selected_endpoint = \"evaluate\"  # Change this to \"compute_complexity\", \"task_verification\", \"run_task\", or \"evaluate\" as needed\n",
    "endpoint_url = endpoint_options[selected_endpoint]\n",
    "\n",
    "print(f\"Using endpoint: {endpoint_url}\")\n",
    "\n",
    "# Handle different request payloads based on endpoint\n",
    "if selected_endpoint == \"evaluate\":\n",
    "    # For error evaluation, we need to specify results file path or results data\n",
    "    # Try the larger results file that might contain failures\n",
    "    results_file_path = \"results/tool-calling-o4-mini_range_0--1_user-task_finance-example-2_llm_2025-09-05_16-13-26.json\"\n",
    "    \n",
    "    # Read the results data from file (since Docker container can't access host files)\n",
    "    if os.path.exists(results_file_path):\n",
    "        with open(results_file_path, 'r') as f:\n",
    "            results_data = json.load(f)\n",
    "        print(f\"Loaded results data from: {results_file_path}\")\n",
    "        print(f\"Results contain {len(results_data)} entries\")\n",
    "        \n",
    "        # Let's peek at the structure to see if there are failures\n",
    "        if results_data:\n",
    "            sample_result = results_data[0] if isinstance(results_data, list) else results_data\n",
    "            print(f\"Sample result keys: {list(sample_result.keys()) if isinstance(sample_result, dict) else 'Not a dict'}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Results file not found: {results_file_path}\")\n",
    "        print(\"Available results files:\")\n",
    "        if os.path.exists(\"results\"):\n",
    "            for file in os.listdir(\"results\"):\n",
    "                print(f\"  - {file}\")\n",
    "        results_data = None\n",
    "    \n",
    "    # Create error evaluation request payload - send data inline instead of file path\n",
    "    if results_data is not None:\n",
    "        error_eval_payload = {\n",
    "            \"env\": \"finance\",  # Changed to match the results file environment\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"max_concurrency\": 1,\n",
    "            \"max_num_failed_results\": 10,  # Limit analysis to first 10 failed results\n",
    "            \"results_data\": results_data,  # Send data inline instead of file path\n",
    "            \"task_file_name\": task_path\n",
    "        }\n",
    "        request_payload = error_eval_payload\n",
    "        print(f\"Prepared error evaluation request with inline data\")\n",
    "    else:\n",
    "        print(\"Cannot proceed without results data\")\n",
    "        request_payload = None\n",
    "else:\n",
    "    # For other endpoints, use the original task JSON\n",
    "    request_payload = task_json\n",
    "    if selected_endpoint == \"run_task\":\n",
    "        print(f\"Number of trials: {task_json['num_trials']}\")\n",
    "\n",
    "# Make the API request\n",
    "if request_payload:\n",
    "    response = requests.post(\n",
    "        endpoint_url,\n",
    "        json=request_payload,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    # Handle different response types based on endpoint\n",
    "    if selected_endpoint == \"run_task\" and response.status_code == 200:\n",
    "        # For run-task endpoint, save the downloaded file\n",
    "        \n",
    "        # Create results directory if it doesn't exist\n",
    "        results_dir = \"results\"\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract filename from Content-Disposition header if available\n",
    "        content_disposition = response.headers.get('Content-Disposition', '')\n",
    "        if 'filename=' in content_disposition:\n",
    "            original_filename = content_disposition.split('filename=')[1].strip('\"')\n",
    "        else:\n",
    "            original_filename = None\n",
    "        \n",
    "        # Use the filename from the server's Content-Disposition header if available\n",
    "        if original_filename:\n",
    "            filename = original_filename\n",
    "        else:\n",
    "            # Fallback filename (shouldn't be needed with updated server)\n",
    "            timestamp = datetime.now().strftime(\"%m%d%H%M%S\")\n",
    "            \n",
    "            # Extract task information to create meaningful filename\n",
    "            task_info = \"\"\n",
    "            if task_json.get('environment'):\n",
    "                env_name = task_json['environment'].replace('_', '-')\n",
    "                task_info = f\"{env_name}_\"\n",
    "            elif 'model' in task_json:\n",
    "                task_info = f\"{task_json['model']}_\"\n",
    "            \n",
    "            filename = f\"tool-calling-{task_info}range_0--1_user-task_llm_{timestamp}.json\"\n",
    "        \n",
    "        # Save the raw file first\n",
    "        raw_file_path = os.path.join(results_dir, filename)\n",
    "        with open(raw_file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Load and display the JSON content as-is (no transformation)\n",
    "        try:\n",
    "            with open(raw_file_path, 'r') as f:\n",
    "                response_data = json.load(f)\n",
    "                print(\"Response JSON loaded successfully\")\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            with open(raw_file_path, 'r') as f:\n",
    "                print(\"First 500 characters of response:\")\n",
    "                print(repr(f.read(500)))\n",
    "            response_data = None\n",
    "            \n",
    "    else:\n",
    "        # For other endpoints, handle as JSON response\n",
    "        try:\n",
    "            response_data = response.json()\n",
    "            print(\"JSON response received\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Response is not valid JSON\")\n",
    "            print(\"Response content:\", response.text[:500])  # Show first 500 chars\n",
    "            response_data = None\n",
    "else:\n",
    "    response_data = None\n",
    "    print(\"No request was made due to missing configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Total results: 1\n",
      "  Failed results: 1\n",
      "  Analyzed results: 1\n",
      "\n",
      "Fault Distribution:\n",
      "  User: 1 (100.0%)\n",
      "  Agent: 0 (0.0%)\n",
      "  Environment: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED ERROR IDENTIFICATION RESPONSES\n",
      "================================================================================\n",
      "\n",
      "ðŸ” FAULT ASSIGNMENT ANALYSIS (1 failures analyzed)\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Task 0: USER FAULT\n",
      "    Explanation: The user is responsible for the fault in the trajectory because they initiated an action to update the size of an existing fund, \"Sanders, Miller and Murphy Growth Fund,\" which was not part of the original instruction. The instruction required the user to create a new fund called \"Tech Growth Fund\" and perform specific operations related to that new fund, not to update an existing fund. The user's actions deviated from the intended task sequence, leading to the fault.\n"
     ]
    }
   ],
   "source": [
    "# Display the raw response data exactly as received\n",
    "if response_data is not None:\n",
    "    if selected_endpoint == \"evaluate\":\n",
    "        # Special handling for error evaluation responses\n",
    "        if response_data.get(\"success\"):\n",
    "            summary = response_data.get(\"summary\", {})\n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"  Total results: {summary.get('total_results', 0)}\")\n",
    "            print(f\"  Failed results: {summary.get('failed_results', 0)}\")\n",
    "            print(f\"  Analyzed results: {summary.get('analyzed_results', 0)}\")\n",
    "            \n",
    "            fault_dist = summary.get(\"fault_distribution\", {})\n",
    "            if fault_dist:\n",
    "                print(f\"\\nFault Distribution:\")\n",
    "                for fault_type, data in fault_dist.items():\n",
    "                    print(f\"  {fault_type.capitalize()}: {data.get('count', 0)} ({data.get('percentage', 0)}%)\")\n",
    "            \n",
    "            fault_type_dist = summary.get(\"fault_type_distribution\", {})\n",
    "            if fault_type_dist:\n",
    "                print(f\"\\nFault Type Distribution (Agent-caused failures only):\")\n",
    "                for fault_type, data in fault_type_dist.items():\n",
    "                    print(f\"  {fault_type.replace('_', ' ').title()}: {data.get('count', 0)} ({data.get('percentage', 0)}%)\")\n",
    "            \n",
    "            # Detailed error identification responses - the actual LLM explanations\n",
    "            fault_assignment = response_data.get(\"fault_assignment_analysis\", [])\n",
    "            fault_type_analysis = response_data.get(\"fault_type_analysis\", [])\n",
    "            \n",
    "            if fault_assignment or fault_type_analysis:\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"DETAILED ERROR IDENTIFICATION RESPONSES\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "                if fault_assignment:\n",
    "                    print(f\"\\nðŸ” FAULT ASSIGNMENT ANALYSIS ({len(fault_assignment)} failures analyzed)\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(fault_assignment, 1):\n",
    "                        task_id = result.get('task_id')\n",
    "                        author = result.get('author', 'unknown')\n",
    "                        description = result.get('description', 'No description available')\n",
    "                        \n",
    "                        print(f\"\\n[{i}] Task {task_id}: {author.upper()} FAULT\")\n",
    "                        print(f\"    Explanation: {description}\")\n",
    "                \n",
    "                if fault_type_analysis:\n",
    "                    print(f\"\\nðŸ”§ FAULT TYPE ANALYSIS ({len(fault_type_analysis)} agent-caused failures analyzed)\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(fault_type_analysis, 1):\n",
    "                        task_id = result.get('task_id')\n",
    "                        fault_type = result.get('fault_type', 'unknown')\n",
    "                        description = result.get('description', 'No description available')\n",
    "                        \n",
    "                        print(f\"\\n[{i}] Task {task_id}: {fault_type.replace('_', ' ').upper()}\")\n",
    "                        print(f\"    Explanation: {description}\")\n",
    "            else:\n",
    "                print(\"\\nâ„¹ï¸  No failed results found in the data to analyze.\")\n",
    "                print(\"   Possible reasons:\")\n",
    "                print(\"   1. The results file contains only successful task executions (reward = 1)\")\n",
    "                print(\"   2. The environment tasks are not available for comparison\")\n",
    "                print(\"   3. Task ID matching failed\")\n",
    "                print(\"   To see error identification in action, you need results with failed tasks (reward = 0)\")\n",
    "                print(\"   and matching environment task definitions.\")\n",
    "                \n",
    "        else:\n",
    "            error_msg = response_data.get('error', 'Unknown error')\n",
    "            print(f\"âŒ Error evaluation failed: {error_msg}\")\n",
    "    else:\n",
    "        # For other endpoints, show raw JSON\n",
    "        print(json.dumps(response_data, indent=2))\n",
    "        \n",
    "else:\n",
    "    print(\"No response data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - tool-calling-gpt-4o-0.0_range_0--1_user-gpt-4o-llm_0827194618.json (28261 bytes)\n",
      "  - tool-calling-o4-mini_range_0--1_user-task_finance-example-2_llm_2025-09-05_16-13-26.json (26806 bytes)\n",
      "  - tool-calling-o4-mini_range_0--1_user-task_finance-example-2_llm_2025-09-05_21-39-35.json (27456 bytes)\n",
      "  - tool-calling-qwen-max_range_0--1_user-task_finance-example-2_llm_2025-09-05_21-34-59.json (199946 bytes)\n",
      "=== DETAILED ERROR ANALYSIS ===\n",
      "\n",
      "=== ALL FAULT ASSIGNMENTS (1 total) ===\n",
      "Task 0: user fault\n",
      "  Description: The user is responsible for the fault in the trajectory because they initiated an action to update the size of an existing fund, \"Sanders, Miller and Murphy Growth Fund,\" which was not part of the original instruction. The instruction required the user to create a new fund called \"Tech Growth Fund\" and perform specific operations related to that new fund, not to update an existing fund. The user's actions deviated from the intended task sequence, leading to the fault.\n",
      "\n",
      "=== FAULT DISTRIBUTION CHART ===\n",
      "User         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|   1 (100.0%)\n",
      "Agent        |â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|   0 (  0.0%)\n",
      "Environment  |â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘|   0 (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results\"\n",
    "if os.path.exists(results_dir):\n",
    "    files = os.listdir(results_dir)\n",
    "    if files:\n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(results_dir, file)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"  - {file} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found\")\n",
    "else:\n",
    "    print(\"Results directory does not exist\")\n",
    "\n",
    "# Check if we have response_data from the previous cell\n",
    "if 'response_data' in globals() and response_data is not None:\n",
    "    \n",
    "    # For compute_complexity and task_verification endpoints - look for plots\n",
    "    if selected_endpoint in [\"compute_complexity\", \"task_verification\"]:\n",
    "        if isinstance(response_data, dict) and 'plot_base64' in response_data and response_data['plot_base64'] is not None:\n",
    "            from IPython.display import Image, display\n",
    "            import base64\n",
    "            \n",
    "            plot_base64 = response_data['plot_base64']\n",
    "            plot_image = base64.b64decode(plot_base64)\n",
    "            display(Image(data=plot_image))\n",
    "        else:\n",
    "            print(\"No plot data found in response\")\n",
    "    \n",
    "    # For run_task endpoint - show raw structure\n",
    "    elif selected_endpoint == \"run_task\":\n",
    "        print(\"Raw API response structure displayed above\")\n",
    "    \n",
    "    # For evaluate endpoint - show detailed analysis\n",
    "    elif selected_endpoint == \"evaluate\":\n",
    "        print(\"=== DETAILED ERROR ANALYSIS ===\")\n",
    "        if response_data.get(\"success\"):\n",
    "            fault_assignment = response_data.get(\"fault_assignment_analysis\", [])\n",
    "            fault_type_analysis = response_data.get(\"fault_type_analysis\", [])\n",
    "            \n",
    "            if fault_assignment:\n",
    "                print(f\"\\n=== ALL FAULT ASSIGNMENTS ({len(fault_assignment)} total) ===\")\n",
    "                for result in fault_assignment:\n",
    "                    print(f\"Task {result.get('task_id')}: {result.get('author')} fault\")\n",
    "                    print(f\"  Description: {result.get('description')}\")\n",
    "                    print()\n",
    "            \n",
    "            if fault_type_analysis:\n",
    "                print(f\"\\n=== ALL FAULT TYPES ({len(fault_type_analysis)} total) ===\")\n",
    "                for result in fault_type_analysis:\n",
    "                    print(f\"Task {result.get('task_id')}: {result.get('fault_type')}\")\n",
    "                    print(f\"  Description: {result.get('description')}\")\n",
    "                    print()\n",
    "            \n",
    "            # Create a simple visualization of fault distribution\n",
    "            summary = response_data.get(\"summary\", {})\n",
    "            fault_dist = summary.get(\"fault_distribution\", {})\n",
    "            if fault_dist:\n",
    "                print(\"=== FAULT DISTRIBUTION CHART ===\")\n",
    "                max_count = max([data.get('count', 0) for data in fault_dist.values()]) if fault_dist else 1\n",
    "                for fault_type, data in fault_dist.items():\n",
    "                    count = data.get('count', 0)\n",
    "                    percentage = data.get('percentage', 0)\n",
    "                    bar_length = int((count / max_count) * 20) if max_count > 0 else 0\n",
    "                    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (20 - bar_length)\n",
    "                    print(f\"{fault_type.capitalize():12} |{bar}| {count:3d} ({percentage:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"Error in analysis: {response_data.get('error', 'Unknown error')}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No response data available from previous cell\")\n",
    "    print(\"Please run the previous cells first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
