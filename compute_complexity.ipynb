{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint: http://localhost:8000/evaluate\n",
      "Loaded results data from: example/result.json\n",
      "Results contain 1 entries\n",
      "Sample result keys: ['task_id', 'reward', 'info', 'traj', 'trial']\n",
      "Prepared error evaluation request with inline data\n",
      "JSON response received\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "task_path = \"example/fund_finance_example.json\"\n",
    "\n",
    "with open(task_path, 'r') as file:\n",
    "    task_json = json.load(file)\n",
    "\n",
    "# Add the task file name to the request so the server can include it in the filename\n",
    "task_json['task_file_name'] = task_path\n",
    "\n",
    "# Configure number of trials/retries for task execution\n",
    "# Default is 1, but you can increase for more robust results\n",
    "task_json['num_trials'] = 1  # Change this to desired number of trials\n",
    "\n",
    "API_BASE_URL= \"https://tau-bench.turing.com\"\n",
    "# API_BASE_URL= \"http://localhost:8000\"\n",
    "\n",
    "# Choose which endpoint to use\n",
    "endpoint_options = {\n",
    "    \"compute_complexity\": f\"{API_BASE_URL}/compute_complexity\",\n",
    "    \"task_verification\": f\"{API_BASE_URL}/task_verification\", \n",
    "    \"run_task\": f\"{API_BASE_URL}/run-task\",\n",
    "    \"evaluate\": f\"{API_BASE_URL}/evaluate\"\n",
    "}\n",
    "\n",
    "# Select the endpoint you want to use\n",
    "selected_endpoint = \"compute_complexity\"  # Change this to \"compute_complexity\", \"task_verification\", \"run_task\", or \"evaluate\" as needed\n",
    "endpoint_url = endpoint_options[selected_endpoint]\n",
    "\n",
    "print(f\"Using endpoint: {endpoint_url}\")\n",
    "\n",
    "# Handle different request payloads based on endpoint\n",
    "if selected_endpoint == \"evaluate\":\n",
    "    # For error evaluation, we need to specify results file path or results data\n",
    "    # Look for result.json in the same directory as the task file\n",
    "    task_dir = os.path.dirname(task_path)\n",
    "    results_file_path = os.path.join(task_dir, \"result.json\")\n",
    "    \n",
    "    # Read the results data from file (since Docker container can't access host files)\n",
    "    if os.path.exists(results_file_path):\n",
    "        with open(results_file_path, 'r') as f:\n",
    "            results_data = json.load(f)\n",
    "        print(f\"Loaded results data from: {results_file_path}\")\n",
    "        print(f\"Results contain {len(results_data)} entries\")\n",
    "        \n",
    "        # Let's peek at the structure to see if there are failures\n",
    "        if results_data:\n",
    "            sample_result = results_data[0] if isinstance(results_data, list) else results_data\n",
    "            print(f\"Sample result keys: {list(sample_result.keys()) if isinstance(sample_result, dict) else 'Not a dict'}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Results file not found: {results_file_path}\")\n",
    "        print(\"Available result files in task directory:\")\n",
    "        task_dir = os.path.dirname(task_path)\n",
    "        if os.path.exists(task_dir):\n",
    "            for file in os.listdir(task_dir):\n",
    "                if file.endswith('.json'):\n",
    "                    print(f\"  - {file}\")\n",
    "        results_data = None\n",
    "    \n",
    "    # Create error evaluation request payload - send data inline instead of file path\n",
    "    if results_data is not None:\n",
    "        error_eval_payload = {\n",
    "            \"env\": \"fund_finance\",  # Changed to match the results file environment\n",
    "            \"model_provider\": \"openai\",\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"max_concurrency\": 1,\n",
    "            \"max_num_failed_results\": 10,  # Limit analysis to first 10 failed results\n",
    "            \"results_data\": results_data,  # Send data inline instead of file path\n",
    "            \"task_file_name\": task_path\n",
    "        }\n",
    "        request_payload = error_eval_payload\n",
    "        print(f\"Prepared error evaluation request with inline data\")\n",
    "    else:\n",
    "        print(\"Cannot proceed without results data\")\n",
    "        request_payload = None\n",
    "else:\n",
    "    # For other endpoints, use the original task JSON\n",
    "    request_payload = task_json\n",
    "    if selected_endpoint == \"run_task\":\n",
    "        print(f\"Number of trials: {task_json['num_trials']}\")\n",
    "\n",
    "# Make the API request\n",
    "if request_payload:\n",
    "    response = requests.post(\n",
    "        endpoint_url,\n",
    "        json=request_payload,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    # Handle different response types based on endpoint\n",
    "    if selected_endpoint == \"run_task\" and response.status_code == 200:\n",
    "        # For run-task endpoint, save the downloaded file\n",
    "        \n",
    "        # Save in the same directory as the task file\n",
    "        task_dir = os.path.dirname(task_path)\n",
    "        os.makedirs(task_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract filename from Content-Disposition header if available\n",
    "        content_disposition = response.headers.get('Content-Disposition', '')\n",
    "        if 'filename=' in content_disposition:\n",
    "            original_filename = content_disposition.split('filename=')[1].strip('\"')\n",
    "        else:\n",
    "            original_filename = None\n",
    "        \n",
    "        # Use the filename from the server's Content-Disposition header if available\n",
    "        if original_filename:\n",
    "            filename = original_filename\n",
    "        else:\n",
    "            # Use standard filename\n",
    "            filename = \"result.json\"\n",
    "        \n",
    "        # Save the raw file first\n",
    "        raw_file_path = os.path.join(task_dir, \"result.json\")\n",
    "        with open(raw_file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # Load and display the JSON content as-is (no transformation)\n",
    "        try:\n",
    "            with open(raw_file_path, 'r') as f:\n",
    "                response_data = json.load(f)\n",
    "                print(\"Response JSON loaded successfully\")\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "            with open(raw_file_path, 'r') as f:\n",
    "                print(\"First 500 characters of response:\")\n",
    "                print(repr(f.read(500)))\n",
    "            response_data = None\n",
    "            \n",
    "    else:\n",
    "        # For other endpoints, handle as JSON response\n",
    "        try:\n",
    "            response_data = response.json()\n",
    "            print(\"JSON response received\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Response is not valid JSON\")\n",
    "            print(\"Response content:\", response.text[:500])  # Show first 500 chars\n",
    "            response_data = None\n",
    "else:\n",
    "    response_data = None\n",
    "    print(\"No request was made due to missing configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "  Total results: 1\n",
      "  Failed results: 1\n",
      "  Analyzed results: 1\n",
      "\n",
      "Fault Distribution:\n",
      "  User: 0 (0.0%)\n",
      "  Agent: 1 (100.0%)\n",
      "  Environment: 0 (0.0%)\n",
      "\n",
      "Fault Type Distribution (Agent-caused failures only):\n",
      "  Called Wrong Tool: 0 (0.0%)\n",
      "  Used Wrong Tool Argument: 1 (100.0%)\n",
      "  Goal Partially Completed: 0 (0.0%)\n",
      "  Other: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "DETAILED ERROR IDENTIFICATION RESPONSES\n",
      "================================================================================\n",
      "\n",
      "üîç FAULT ASSIGNMENT ANALYSIS (1 failures analyzed)\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Task 0: AGENT FAULT\n",
      "    Explanation: The agent is responsible for the fault in the trajectory because it failed to perform the correct actions as per the ground truth action sequence. The agent should have executed an \"approval_lookup\" action with the email \"admin@company.com\" for system monitoring, followed by generating a \"system_overview\" report. Instead, the agent repeatedly attempted to verify authorization using the user's provided email, which was not part of the required actions, and did not proceed with the necessary system monitoring and report generation tasks.\n",
      "\n",
      "üîß FAULT TYPE ANALYSIS (1 agent-caused failures analyzed)\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1] Task 0: USED WRONG TOOL ARGUMENT\n",
      "    Explanation: The fault in the trajectory is that the assistant repeatedly attempts to verify the user's authorization using the email \"maria.rodriguez@fundmanagers.com\" for fund management setup, which is not the correct action according to the ground truth. The ground truth specifies that the \"approval_lookup\" action should be performed with the \"action\" argument set to \"system_monitoring\" and the \"requester_email\" set to \"admin@company.com\". The trajectory incorrectly uses the email \"maria.rodriguez@fundmanagers.com\" and does not perform the \"system_monitoring\" action, leading to the fault of using the wrong tool argument.\n"
     ]
    }
   ],
   "source": [
    "# Display the raw response data exactly as received\n",
    "if response_data is not None:\n",
    "    if selected_endpoint == \"evaluate\":\n",
    "        # Special handling for error evaluation responses\n",
    "        if response_data.get(\"success\"):\n",
    "            summary = response_data.get(\"summary\", {})\n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"  Total results: {summary.get('total_results', 0)}\")\n",
    "            print(f\"  Failed results: {summary.get('failed_results', 0)}\")\n",
    "            print(f\"  Analyzed results: {summary.get('analyzed_results', 0)}\")\n",
    "            \n",
    "            fault_dist = summary.get(\"fault_distribution\", {})\n",
    "            if fault_dist:\n",
    "                print(f\"\\nFault Distribution:\")\n",
    "                for fault_type, data in fault_dist.items():\n",
    "                    print(f\"  {fault_type.capitalize()}: {data.get('count', 0)} ({data.get('percentage', 0)}%)\")\n",
    "            \n",
    "            fault_type_dist = summary.get(\"fault_type_distribution\", {})\n",
    "            if fault_type_dist:\n",
    "                print(f\"\\nFault Type Distribution (Agent-caused failures only):\")\n",
    "                for fault_type, data in fault_type_dist.items():\n",
    "                    print(f\"  {fault_type.replace('_', ' ').title()}: {data.get('count', 0)} ({data.get('percentage', 0)}%)\")\n",
    "            \n",
    "            # Detailed error identification responses - the actual LLM explanations\n",
    "            fault_assignment = response_data.get(\"fault_assignment_analysis\", [])\n",
    "            fault_type_analysis = response_data.get(\"fault_type_analysis\", [])\n",
    "            \n",
    "            if fault_assignment or fault_type_analysis:\n",
    "                print(\"\\n\" + \"=\" * 80)\n",
    "                print(\"DETAILED ERROR IDENTIFICATION RESPONSES\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "                if fault_assignment:\n",
    "                    print(f\"\\nüîç FAULT ASSIGNMENT ANALYSIS ({len(fault_assignment)} failures analyzed)\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(fault_assignment, 1):\n",
    "                        task_id = result.get('task_id')\n",
    "                        author = result.get('author', 'unknown')\n",
    "                        description = result.get('description', 'No description available')\n",
    "                        \n",
    "                        print(f\"\\n[{i}] Task {task_id}: {author.upper()} FAULT\")\n",
    "                        print(f\"    Explanation: {description}\")\n",
    "                \n",
    "                if fault_type_analysis:\n",
    "                    print(f\"\\nüîß FAULT TYPE ANALYSIS ({len(fault_type_analysis)} agent-caused failures analyzed)\")\n",
    "                    print(\"-\" * 60)\n",
    "                    for i, result in enumerate(fault_type_analysis, 1):\n",
    "                        task_id = result.get('task_id')\n",
    "                        fault_type = result.get('fault_type', 'unknown')\n",
    "                        description = result.get('description', 'No description available')\n",
    "                        \n",
    "                        print(f\"\\n[{i}] Task {task_id}: {fault_type.replace('_', ' ').upper()}\")\n",
    "                        print(f\"    Explanation: {description}\")\n",
    "            else:\n",
    "                print(\"\\n‚ÑπÔ∏è  No failed results found in the data to analyze.\")\n",
    "                print(\"   Possible reasons:\")\n",
    "                print(\"   1. The results file contains only successful task executions (reward = 1)\")\n",
    "                print(\"   2. The environment tasks are not available for comparison\")\n",
    "                print(\"   3. Task ID matching failed\")\n",
    "                print(\"   To see error identification in action, you need results with failed tasks (reward = 0)\")\n",
    "                print(\"   and matching environment task definitions.\")\n",
    "                \n",
    "        else:\n",
    "            error_msg = response_data.get('error', 'Unknown error')\n",
    "            print(f\"‚ùå Error evaluation failed: {error_msg}\")\n",
    "    else:\n",
    "        # For other endpoints, show raw JSON\n",
    "        print(json.dumps(response_data, indent=2))\n",
    "        \n",
    "else:\n",
    "    print(\"No response data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legacy results directory does not exist\n",
      "\n",
      "Task result file: example/result.json (48390 bytes)\n",
      "=== DETAILED ERROR ANALYSIS ===\n",
      "\n",
      "=== ALL FAULT ASSIGNMENTS (1 total) ===\n",
      "Task 0: agent fault\n",
      "  Description: The agent is responsible for the fault in the trajectory because it failed to perform the correct actions as per the ground truth action sequence. The agent should have executed an \"approval_lookup\" action with the email \"admin@company.com\" for system monitoring, followed by generating a \"system_overview\" report. Instead, the agent repeatedly attempted to verify authorization using the user's provided email, which was not part of the required actions, and did not proceed with the necessary system monitoring and report generation tasks.\n",
      "\n",
      "\n",
      "=== ALL FAULT TYPES (1 total) ===\n",
      "Task 0: used_wrong_tool_argument\n",
      "  Description: The fault in the trajectory is that the assistant repeatedly attempts to verify the user's authorization using the email \"maria.rodriguez@fundmanagers.com\" for fund management setup, which is not the correct action according to the ground truth. The ground truth specifies that the \"approval_lookup\" action should be performed with the \"action\" argument set to \"system_monitoring\" and the \"requester_email\" set to \"admin@company.com\". The trajectory incorrectly uses the email \"maria.rodriguez@fundmanagers.com\" and does not perform the \"system_monitoring\" action, leading to the fault of using the wrong tool argument.\n",
      "\n",
      "=== FAULT DISTRIBUTION CHART ===\n",
      "User         |‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë|   0 (  0.0%)\n",
      "Agent        |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|   1 (100.0%)\n",
      "Environment  |‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë|   0 (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"results\"\n",
    "if os.path.exists(results_dir):\n",
    "    files = os.listdir(results_dir)\n",
    "    if files:\n",
    "        print(\"Legacy results directory:\")\n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(results_dir, file)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"  - {file} ({file_size} bytes)\")\n",
    "    else:\n",
    "        print(\"No files found in legacy results directory\")\n",
    "else:\n",
    "    print(\"Legacy results directory does not exist\")\n",
    "\n",
    "# Also check for result.json in the task directory\n",
    "task_dir = os.path.dirname(task_path) if 'task_path' in globals() else \"example\"\n",
    "task_result_file = os.path.join(task_dir, \"result.json\")\n",
    "if os.path.exists(task_result_file):\n",
    "    file_size = os.path.getsize(task_result_file)\n",
    "    print(f\"\\nTask result file: {task_result_file} ({file_size} bytes)\")\n",
    "else:\n",
    "    print(f\"\\nNo result.json found in task directory: {task_dir}\")\n",
    "\n",
    "# Check if we have response_data from the previous cell\n",
    "if 'response_data' in globals() and response_data is not None:\n",
    "    \n",
    "    # For compute_complexity and task_verification endpoints - look for plots\n",
    "    if selected_endpoint in [\"compute_complexity\", \"task_verification\"]:\n",
    "        if isinstance(response_data, dict) and 'plot_base64' in response_data and response_data['plot_base64'] is not None:\n",
    "            from IPython.display import Image, display\n",
    "            import base64\n",
    "            \n",
    "            plot_base64 = response_data['plot_base64']\n",
    "            plot_image = base64.b64decode(plot_base64)\n",
    "            display(Image(data=plot_image))\n",
    "        else:\n",
    "            print(\"No plot data found in response\")\n",
    "    \n",
    "    # For run_task endpoint - show raw structure\n",
    "    elif selected_endpoint == \"run_task\":\n",
    "        print(\"Raw API response structure displayed above\")\n",
    "    \n",
    "    # For evaluate endpoint - show detailed analysis\n",
    "    elif selected_endpoint == \"evaluate\":\n",
    "        print(\"=== DETAILED ERROR ANALYSIS ===\")\n",
    "        if response_data.get(\"success\"):\n",
    "            fault_assignment = response_data.get(\"fault_assignment_analysis\", [])\n",
    "            fault_type_analysis = response_data.get(\"fault_type_analysis\", [])\n",
    "            \n",
    "            if fault_assignment:\n",
    "                print(f\"\\n=== ALL FAULT ASSIGNMENTS ({len(fault_assignment)} total) ===\")\n",
    "                for result in fault_assignment:\n",
    "                    print(f\"Task {result.get('task_id')}: {result.get('author')} fault\")\n",
    "                    print(f\"  Description: {result.get('description')}\")\n",
    "                    print()\n",
    "            \n",
    "            if fault_type_analysis:\n",
    "                print(f\"\\n=== ALL FAULT TYPES ({len(fault_type_analysis)} total) ===\")\n",
    "                for result in fault_type_analysis:\n",
    "                    print(f\"Task {result.get('task_id')}: {result.get('fault_type')}\")\n",
    "                    print(f\"  Description: {result.get('description')}\")\n",
    "                    print()\n",
    "            \n",
    "            # Create a simple visualization of fault distribution\n",
    "            summary = response_data.get(\"summary\", {})\n",
    "            fault_dist = summary.get(\"fault_distribution\", {})\n",
    "            if fault_dist:\n",
    "                print(\"=== FAULT DISTRIBUTION CHART ===\")\n",
    "                max_count = max([data.get('count', 0) for data in fault_dist.values()]) if fault_dist else 1\n",
    "                for fault_type, data in fault_dist.items():\n",
    "                    count = data.get('count', 0)\n",
    "                    percentage = data.get('percentage', 0)\n",
    "                    bar_length = int((count / max_count) * 20) if max_count > 0 else 0\n",
    "                    bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "                    print(f\"{fault_type.capitalize():12} |{bar}| {count:3d} ({percentage:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"Error in analysis: {response_data.get('error', 'Unknown error')}\")\n",
    "            \n",
    "else:\n",
    "    print(\"No response data available from previous cell\")\n",
    "    print(\"Please run the previous cells first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
